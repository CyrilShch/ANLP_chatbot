{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Message Bot_Transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"z-E0tDo2a3D5"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import sys\n","\n","import tensorflow as tf\n","\n","tf.random.set_seed(1234)\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","import tensorflow_datasets as tfds\n","\n","import os\n","import re\n","import numpy as np\n","from time import time\n","import matplotlib.pyplot as plt\n","\n","print(\"Tensorflow version {}\".format(tf.__version__))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKMs7UE8ag_U"},"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thw8UmJeZ00u"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOiRH0gIdPSw"},"source":["# Hyper-parameters\n","#Preprocessing\n","MAX_LENGTH = 30  # Maximum sentence length to consider\n","MIN_COUNT = 1    # Minimum word count threshold for trimming\n","\n","#Training and validation data\n","#BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 200000\n","VALIDATION_SPLIT = 0.9\n","\n","#Transformer\n","NUM_LAYERS = 6\n","D_MODEL = 512\n","NUM_HEADS = 8\n","UNITS = 2048\n","DROPOUT = 0.2\n","\n","#Training\n","EPOCHS = 80"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcfxQZfSATKs"},"source":["!wget -nc http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n","!wget -nc https://www.dropbox.com/s/jf72i3f73o2lfoi/chatbot_data.zip\n","!unzip -o cornell_movie_dialogs_corpus.zip\n","!unzip -o chatbot_data.zip\n","\n","import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import csv\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","\n","USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","corpus_name = \"cornell movie-dialogs corpus\"\n","corpus = corpus_name\n","\n","def printLines(file, n=10):\n","    with open(file, 'rb') as datafile:\n","        lines = datafile.readlines()\n","    for line in lines[:n]:\n","        print(line)\n","\n","# printLines(os.path.join(corpus, \"movie_lines.txt\"))\n","\n","# Splits each line of the file into a dictionary of fields\n","def loadLines(fileName, fields):\n","    lines = {}\n","    with open(fileName, 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            values = line.split(\" +++$+++ \")\n","            # Extract fields\n","            lineObj = {}\n","            for i, field in enumerate(fields):\n","                lineObj[field] = values[i]\n","            lines[lineObj['lineID']] = lineObj\n","    return lines\n","\n","\n","# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n","def loadConversations(fileName, lines, fields):\n","    conversations = []\n","    with open(fileName, 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            values = line.split(\" +++$+++ \")\n","            # Extract fields\n","            convObj = {}\n","            for i, field in enumerate(fields):\n","                convObj[field] = values[i]\n","            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n","            utterance_id_pattern = re.compile('L[0-9]+')\n","            lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"])\n","            # Reassemble lines\n","            convObj[\"lines\"] = []\n","            for lineId in lineIds:\n","                convObj[\"lines\"].append(lines[lineId])\n","            conversations.append(convObj)\n","    return conversations\n","\n","\n","# Extracts pairs of sentences from conversations\n","def extractSentencePairs(conversations):\n","    qa_pairs = []\n","    for conversation in conversations:\n","        # Iterate over all the lines of the conversation\n","        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n","            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n","            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n","            # Filter wrong samples (if one of the lists is empty)\n","            if inputLine and targetLine:\n","                qa_pairs.append([inputLine, targetLine])\n","    return qa_pairs\n","\n","\n","# Define path to new file\n","datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n","\n","delimiter = '\\t'\n","# Unescape the delimiter\n","delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n","\n","# Initialize lines dict, conversations list, and field ids\n","lines = {}\n","conversations = []\n","MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n","MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n","\n","# Load lines and process conversations\n","print(\"\\nProcessing corpus...\")\n","lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n","print(\"\\nLoading conversations...\")\n","conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n","                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n","\n","# Write new csv file\n","print(\"\\nWriting newly formatted file...\")\n","with open(datafile, 'w', encoding='utf-8') as outputfile:\n","    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n","    for pair in extractSentencePairs(conversations):\n","        writer.writerow(pair)\n","\n","# # Print a sample of lines\n","# print(\"\\nSample lines from file:\")\n","# printLines(datafile)\n","\n","\n","# Default word tokens\n","PAD_token = 0  # Used for padding short sentences\n","SOS_token = 1  # Start-of-sentence token\n","EOS_token = 2  # End-of-sentence token\n","\n","class Voc:\n","    def __init__(self, name):\n","        self.name = name\n","        self.trimmed = False\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3  # Count SOS, EOS, PAD\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.num_words\n","            self.word2count[word] = 1\n","            self.index2word[self.num_words] = word\n","            self.num_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    # Remove words below a certain count threshold\n","    def trim(self, min_count):\n","        if self.trimmed:\n","            return\n","        self.trimmed = True\n","\n","        keep_words = []\n","\n","        for k, v in self.word2count.items():\n","            if v >= min_count:\n","                keep_words.append(k)\n","\n","        print('keep_words {} / {} = {:.4f}'.format(\n","            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n","        ))\n","\n","        # Reinitialize dictionaries\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3 # Count default tokens\n","\n","        for word in keep_words:\n","            self.addWord(word)\n","\n","\n","# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(sentence):\n","    sentence = sentence.lower().strip()\n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\"\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    # removing contractions\n","    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n","    sentence = re.sub(r\"he's\", \"he is\", sentence)\n","    sentence = re.sub(r\"she's\", \"she is\", sentence)\n","    sentence = re.sub(r\"it's\", \"it is\", sentence)\n","    sentence = re.sub(r\"that's\", \"that is\", sentence)\n","    sentence = re.sub(r\"what's\", \"that is\", sentence)\n","    sentence = re.sub(r\"where's\", \"where is\", sentence)\n","    sentence = re.sub(r\"how's\", \"how is\", sentence)\n","    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n","    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n","    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n","    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n","    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n","    sentence = re.sub(r\"won't\", \"will not\", sentence)\n","    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n","    sentence = re.sub(r\"n't\", \" not\", sentence)\n","    sentence = re.sub(r\"n'\", \"ng\", sentence)\n","    sentence = re.sub(r\"'bout\", \"about\", sentence)\n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n","    sentence = sentence.strip()\n","    return sentence\n","\n","# Read query/response pairs and return a voc object\n","def readVocs(datafile, corpus_name):\n","    print(\"Reading lines...\")\n","    # Read the file and split into lines\n","    lines = open(datafile, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","    voc = Voc(corpus_name)\n","    return voc, pairs\n","\n","# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n","def filterPair(p):\n","    # Input sequences need to preserve the last word for EOS token\n","    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","\n","# Filter pairs using filterPair condition\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","# Using the functions defined above, return a populated voc object and pairs list\n","def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n","    print(\"Start preparing training data ...\")\n","    voc, pairs = readVocs(datafile, corpus_name)\n","    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","    print(\"Counted words:\", voc.num_words)\n","    return voc, pairs\n","\n","\n","# Load/Assemble voc and pairs\n","save_dir = os.path.join(\"data\", \"save\")\n","voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n","# Print some pairs to validate\n","# print(\"\\npairs:\")\n","# for pair in pairs[:10]:\n","#     print(pair)\n","\n","\n","def trimRareWords(voc, pairs, MIN_COUNT):\n","    # Trim words used under the MIN_COUNT from the voc\n","    voc.trim(MIN_COUNT)\n","    # Filter out pairs with trimmed words\n","    keep_pairs = []\n","    for pair in pairs:\n","        input_sentence = pair[0]\n","        output_sentence = pair[1]\n","        keep_input = True\n","        keep_output = True\n","        # Check input sentence\n","        for word in input_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_input = False\n","                break\n","        # Check output sentence\n","        for word in output_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_output = False\n","                break\n","\n","        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","\n","    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n","    return keep_pairs\n","\n","\n","# Trim voc and pairs\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)\n","\n","import pickle\n","\n","### Load full tennis pairs\n","with open('/content/chatbot_data/tennis_pairs_full.txt', 'rb') as fp:\n","    tennis_full_pairs = pickle.load(fp)\n","\n","# ### Load modified 1 tennis pairs\n","# with open('/content/chatbot_data/tennis_pairs_mod1.txt', 'rb') as fp:\n","#     tennis_mod1_pairs = pickle.load(fp)\n","\n","# ### Load modified 2 tennis pairs\n","# with open('/content/chatbot_data/tennis_pairs_mod2.txt', 'rb') as fp:\n","#     tennis_mod2_pairs = pickle.load(fp)\n","\n","### Load full persuasionforgood pairs\n","with open('/content/chatbot_data/persuasionforgood_corpus.txt', 'rb') as fp:\n","    persuasionforgood_full_pairs = pickle.load(fp)\n","\n","# ### Load modified 1 persuasionforgood pairs\n","# with open('/content/chatbot_data/persuasionforgood_pairs_mod1.txt', 'rb') as fp:\n","#     persuasionforgood_mod1_pairs = pickle.load(fp)\n","\n","# ### Load modified 2 persuasionforgood pairs\n","# with open('/content/chatbot_data/persuasionforgood_pairs_mod2.txt', 'rb') as fp:\n","#     persuasionforgood_mod2_pairs = pickle.load(fp)\n","\n","### Load Yandex.toloka dataset 1 pairs\n","with open('/content/chatbot_data/data_tolokers.txt', 'rb') as fp:\n","    yandex_set1_pairs = pickle.load(fp)\n","\n","### Load Yandex.toloka dataset 2 pairs\n","with open('/content/chatbot_data/data_volunteers.txt', 'rb') as fp:\n","    yandex_set2_pairs = pickle.load(fp)\n","\n","### Load Yandex.toloka dataset 3 pairs\n","with open('/content/chatbot_data/data_intermediate.txt', 'rb') as fp:\n","    yandex_set3_pairs = pickle.load(fp)\n","\n","### Load PersonaChat Amazon pairs\n","with open('/content/chatbot_data/personaChat_pairs.txt', 'rb') as fp:\n","    personaChat_pairs = pickle.load(fp)\n","\n","### Load Friends pairs\n","with open('/content/chatbot_data/friends_corpus.txt', 'rb') as fp:\n","    friends_pairs = pickle.load(fp)\n","\n","### check the lengths of datasets\n","print(f\"# of pairs in full tennis pairs: {len(tennis_full_pairs)}\")\n","# print(f\"# of pairs in modified 1 tennis pairs: {len(tennis_mod1_pairs)}\")\n","# print(f\"# of pairs in modified 2 tennis pairs: {len(tennis_mod2_pairs)}\")\n","print(f\"# of pairs in full persuasionforgood pairs: {len(persuasionforgood_full_pairs)}\")\n","# print(f\"# of pairs in modified 1 persuasionforgood pairs: {len(persuasionforgood_mod1_pairs)}\")\n","# print(f\"# of pairs in modified 2 persuasionforgood pairs: {len(persuasionforgood_mod2_pairs)}\")\n","print(f\"# of pairs in Yandex.toloka dataset 1 pairs: {len(yandex_set1_pairs)}\")\n","print(f\"# of pairs in Yandex.toloka dataset 2 pairs: {len(yandex_set2_pairs)}\")\n","print(f\"# of pairs in Yandex.toloka dataset 3 pairs: {len(yandex_set3_pairs)}\")\n","print(f\"# of pairs in PersonaChat Amazon pairs: {len(personaChat_pairs)}\")\n","print(f\"# of pairs in Friends pairs: {len(friends_pairs)}\")\n","\n","\n","### merge all new datasets and normalize them\n","### merge\n","merged_pairs = tennis_full_pairs + persuasionforgood_full_pairs + yandex_set1_pairs + yandex_set2_pairs + yandex_set3_pairs + personaChat_pairs + friends_pairs\n","### normalize \n","merged_pairs = [[normalizeString(s) for s in pair] for pair in merged_pairs]\n","# print(f\"# of a merged new dataset {len(merged_pairs)}\")\n","\n","### filter pairs which maximum sentence length is bigger than the threshold\n","merged_pairs = filterPairs(merged_pairs)\n","print(f\"# of a merged filtered new dataset {len(merged_pairs)}\")\n","\n","new_pairs = merged_pairs + pairs\n","\n","### Update vocabulary and trim all pairs again\n","pairs = new_pairs\n","### Trim words\n","for pair in pairs:\n","    voc.addSentence(pair[0])\n","    voc.addSentence(pair[1])\n","\n","# Trim voc and pairs\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)\n","\n","print(f\"Final # of the pairs: {len(pairs)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5mPJ4tGBsLr"},"source":["def getDataset(pairs):    \n","    questions = []\n","    answers = []\n","\n","    for pair in pairs:\n","        questions.append(pair[0])\n","        answers.append(pair[1])\n","\n","    return questions, answers\n","\n","\n","questions, answers = getDataset(pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQlaWrhBuN4w"},"source":["# Set up IPython to show all outputs from a cell\n","import warnings\n","from IPython.core.interactiveshell import InteractiveShell\n","\n","InteractiveShell.ast_node_interactivity = 'all'\n","\n","warnings.filterwarnings('ignore', category=RuntimeWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQs163CZB201"},"source":["print(questions[100000:100010])\n","print(\"\\n\")\n","answers[100000:100010]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VWBm2e5KA7j"},"source":["tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**25)\n","\n","# Define start and end token to indicate the start and end of a sentence\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","\n","# Vocabulary size plus start and end token\n","VOCAB_SIZE = tokenizer.vocab_size + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynKwrfYiLWzZ"},"source":["questions[21]\n","print('Tokenized sample question: {}'.format(tokenizer.encode(questions[21])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-gAbGiFLsdT"},"source":["# Tokenize, filter and pad sentences\n","def tokenize_and_filter(inputs, outputs):\n","    tokenized_inputs, tokenized_outputs = [], []\n","\n","    for (sentence1, sentence2) in zip(inputs, outputs):\n","        # tokenize sentence and append to input and output\n","        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","        tokenized_inputs.append(sentence1)\n","\n","        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","        tokenized_outputs.append(sentence2)\n","\n","    # pad tokenized sentences\n","    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","\n","    return tokenized_inputs, tokenized_outputs\n","\n","\n","questions, answers = tokenize_and_filter(questions, answers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTXfYTTKMBep"},"source":["print('Vocab size: {}'.format(VOCAB_SIZE))\n","print('Number of samples: {}'.format(len(questions)))\n","len(questions[0]),len(answers[0])\n","print(questions[0])\n","print(answers[0])\n","questions.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ogMDXU1MSD5"},"source":["train_answers = answers[:int(len(answers) * VALIDATION_SPLIT)]\n","train_questions = questions[:int(len(questions) * VALIDATION_SPLIT)]\n","\n","validation_answers = answers[int(len(answers) * VALIDATION_SPLIT):]\n","validation_questions = questions[int(len(questions) * VALIDATION_SPLIT):]\n","\n","# decoder inputs use the previous target as input\n","# remove START_TOKEN from targets\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': train_questions,\n","        'dec_inputs': train_answers[:, :-1]\n","    },\n","    {\n","        'outputs': train_answers[:, 1:]\n","    },\n","))\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': validation_questions,\n","        'dec_inputs': validation_answers[:, :-1]\n","    },\n","    {\n","        'outputs': validation_answers[:, 1:]\n","    },\n","))\n","\n","\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","validation_dataset = validation_dataset.cache()\n","validation_dataset = validation_dataset.shuffle(BUFFER_SIZE)\n","validation_dataset = validation_dataset.batch(BATCH_SIZE)\n","validation_dataset = validation_dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pg-kn227Mjxb"},"source":["print(train_dataset)\n","print(train_answers.shape)\n","print(validation_answers.shape)\n","print(train_questions.shape)\n","print(validation_questions.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbPVaU-dPwMO"},"source":["def scaled_dot_product_attention(query, key, value, mask):\n","    \"\"\"Calculate the attention weights. \"\"\"\n","    matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","    # scale matmul_qk\n","    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","    logits = matmul_qk / tf.math.sqrt(depth)\n","\n","    # add the mask to zero out padding tokens\n","    if mask is not None:\n","        logits += (mask * -1e9)\n","\n","    # softmax is normalized on the last axis (seq_len_k)\n","    attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, value)\n","\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jfAXNBhQV7I"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","        super(MultiHeadAttention, self).__init__(name=name)\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.query_dense = tf.keras.layers.Dense(units=d_model)\n","        self.key_dense = tf.keras.layers.Dense(units=d_model)\n","        self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","        self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","    def split_heads(self, inputs, batch_size):\n","        inputs = tf.reshape(\n","            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","            'value'], inputs['mask']\n","        batch_size = tf.shape(query)[0]\n","\n","        # linear layers\n","        query = self.query_dense(query)\n","        key = self.key_dense(key)\n","        value = self.value_dense(value)\n","\n","        # split heads\n","        query = self.split_heads(query, batch_size)\n","        key = self.split_heads(key, batch_size)\n","        value = self.split_heads(value, batch_size)\n","\n","        # scaled dot-product attention\n","        scaled_attention = scaled_dot_product_attention(\n","            query, key, value, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        # concatenation of heads\n","        concat_attention = tf.reshape(scaled_attention,\n","                                      (batch_size, -1, self.d_model))\n","\n","        # final linear layer\n","        outputs = self.dense(concat_attention)\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypDylOh1QgRV"},"source":["def create_padding_mask(x):\n","    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","    # (batch_size, 1, 1, sequence length)\n","    return mask[:, tf.newaxis, tf.newaxis, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgZerRrWQxKQ"},"source":["def create_look_ahead_mask(x):\n","    seq_len = tf.shape(x)[1]\n","    look_ahead_mask = 1 - \\\n","        tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    padding_mask = create_padding_mask(x)\n","    return tf.maximum(look_ahead_mask, padding_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32PS_NnQSCR9"},"source":["class PositionalEncoding(tf.keras.layers.Layer):\n","\n","    def __init__(self, position, d_model):\n","        super(PositionalEncoding, self).__init__()\n","        self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","    def get_angles(self, position, i, d_model):\n","        angles = 1 / tf.pow(10000, (2 * (i // 2)) /\n","                            tf.cast(d_model, tf.float32))\n","        return position * angles\n","\n","    def positional_encoding(self, position, d_model):\n","        angle_rads = self.get_angles(\n","            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","            d_model=d_model)\n","        # apply sin to even index in the array\n","        sines = tf.math.sin(angle_rads[:, 0::2])\n","        # apply cos to odd index in the array\n","        cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","        pos_encoding = tf.concat([sines, cosines], axis=-1)\n","        pos_encoding = pos_encoding[tf.newaxis, ...]\n","        return tf.cast(pos_encoding, tf.float32)\n","\n","    def call(self, inputs):\n","        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyXmEAJKSLN9"},"source":["def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","    attention = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention\")({\n","            'query': inputs,\n","            'key': inputs,\n","            'value': inputs,\n","            'mask': padding_mask\n","        })\n","    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","    attention = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(inputs + attention)\n","\n","    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n","    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention + outputs)\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, padding_mask], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3Gya33cU1GE"},"source":["def encoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name=\"encoder\"):\n","    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","    for i in range(num_layers):\n","        outputs = encoder_layer(\n","            units=units,\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            name=\"encoder_layer_{}\".format(i),\n","        )([outputs, padding_mask])\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, padding_mask], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BaIEM9KCV3j3"},"source":["def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","    look_ahead_mask = tf.keras.Input(\n","        shape=(1, None, None), name=\"look_ahead_mask\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","    attention1 = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention_1\")(inputs={\n","            'query': inputs,\n","            'key': inputs,\n","            'value': inputs,\n","            'mask': look_ahead_mask\n","        })\n","    attention1 = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention1 + inputs)\n","\n","    attention2 = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention_2\")(inputs={\n","            'query': attention1,\n","            'key': enc_outputs,\n","            'value': enc_outputs,\n","            'mask': padding_mask\n","        })\n","    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","    attention2 = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(attention2 + attention1)\n","\n","    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n","    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6)(outputs + attention2)\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","        outputs=outputs,\n","        name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCI1VnKdW643"},"source":["def decoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name='decoder'):\n","    inputs = tf.keras.Input(shape=(None,), name='inputs')\n","    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","    look_ahead_mask = tf.keras.Input(\n","        shape=(1, None, None), name='look_ahead_mask')\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","    for i in range(num_layers):\n","        outputs = decoder_layer(\n","            units=units,\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            name='decoder_layer_{}'.format(i),\n","        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","        outputs=outputs,\n","        name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"juBTtYZUX4Eu"},"source":["def transformer(vocab_size,\n","                num_layers,\n","                units,\n","                d_model,\n","                num_heads,\n","                dropout,\n","                name=\"transformer\"):\n","    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","    enc_padding_mask = tf.keras.layers.Lambda(\n","        create_padding_mask, output_shape=(1, 1, None),\n","        name='enc_padding_mask')(inputs)\n","    # mask the future tokens for decoder inputs at the 1st attention block\n","    look_ahead_mask = tf.keras.layers.Lambda(\n","        create_look_ahead_mask,\n","        output_shape=(1, None, None),\n","        name='look_ahead_mask')(dec_inputs)\n","    # mask the encoder outputs for the 2nd attention block\n","    dec_padding_mask = tf.keras.layers.Lambda(\n","        create_padding_mask, output_shape=(1, 1, None),\n","        name='dec_padding_mask')(inputs)\n","\n","    enc_outputs = encoder(\n","        vocab_size=vocab_size,\n","        num_layers=num_layers,\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","    )(inputs=[inputs, enc_padding_mask])\n","\n","    dec_outputs = decoder(\n","        vocab_size=vocab_size,\n","        num_layers=num_layers,\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","    outputs = tf.keras.layers.Dense(\n","        units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JiC37YpaYU4B"},"source":["def loss_function(y_true, y_pred):\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')(y_true, y_pred)\n","\n","    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","    loss = tf.multiply(loss, mask)\n","\n","    return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PojrmVzCYfRD"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRfVrCmCYgCM"},"source":["learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","\n","def accuracy(y_true, y_pred):\n","    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-iy0TdKeRQV"},"source":["# initialize and compile model within strategy scope\n","tf.keras.backend.clear_session()\n","\n","with strategy.scope():\n","  model = transformer(\n","      vocab_size=VOCAB_SIZE,\n","      num_layers=NUM_LAYERS,\n","      units=UNITS,\n","      d_model=D_MODEL,\n","      num_heads=NUM_HEADS,\n","      dropout=DROPOUT)\n","\n","  model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQcgt5D4aLRb"},"source":["model_name = 'Transformer_6Layer_30ML'\n","model_dir = '/content/drive/My Drive/NLP/Models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFyXAZtLiBvc"},"source":["#folder to save models\n","#%cd '/content/drive/My Drive/NLP/Models/'\n","# get the model params\n","#hparams = {'vocab_size': VOCAB_SIZE, 'num_layers': NUM_LAYERS, 'units': UNITS, 'd_model':D_MODEL, 'num_heads':NUM_HEADS, 'dropout': DROPOUT}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AJElYJbjGDT"},"source":["import json\n","#with open(f'{model_name}_hparams.json', 'w') as fp:\n","    #json.dump(hparams, fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpyO6XUfaB94"},"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","def make_callbacks(model_name, save=True):\n","    \"\"\"Make list of callbacks for training\"\"\"\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n","\n","    if save:\n","        callbacks.append(\n","            ModelCheckpoint(\n","                f'{model_dir}{model_name}.h5',\n","                save_best_only=True,\n","                save_weights_only=True))\n","    return callbacks\n","\n","callbacks = make_callbacks(model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVpKBOyJYp2H"},"source":["history = model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS, callbacks=[EarlyStopping(monitor='loss', patience=5)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldQQgQtLwzi9"},"source":["import pandas as pd\n","\n","history_df = pd.DataFrame(history.history)\n","history_df['epoch'] = history.epoch\n","fig, axes = plt.subplots(2, sharex=True, figsize=(15, 8))\n","fig.suptitle('Learning Curves')\n","\n","epochs_to_mean = 1\n","\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].plot(history_df['accuracy'].rolling(epochs_to_mean).mean(), 'b')\n","axes[0].plot(history_df['val_accuracy'].rolling(epochs_to_mean).mean(), '-r')\n","axes[0].legend(['Training Accuracy', 'Validation Accuracy'])\n","\n","axes[1].set_ylabel(\"Loss\")\n","axes[1].plot(history_df['loss'].rolling(epochs_to_mean).mean(), 'b')\n","axes[1].plot(history_df['val_loss'].rolling(epochs_to_mean).mean(), '-r')\n","axes[1].legend(['Training Loss', 'Validation Loss'])\n","axes[1].set_xlabel(\"Epoch\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDiDdYaQiUYl"},"source":["# save the model\n","#model.save_weights(f'{model_name}.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tg8Xbn3-rUko"},"source":["#model.load_weights(f'{model_dir}{model_name}.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLeum2D_gd_d"},"source":["#Build model to be loaded for testing\n","model = transformer(\n","      vocab_size=VOCAB_SIZE,\n","      num_layers=NUM_LAYERS,\n","      units=UNITS,\n","      d_model=D_MODEL,\n","      num_heads=NUM_HEADS,\n","      dropout=DROPOUT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw54gS_V02wG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615736359454,"user_tz":-60,"elapsed":16448,"user":{"displayName":"Shashank Subramanya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRTVZDLuIRxowDxaCY2oQp6hQSzC0ROoJKAfvO=s64","userId":"05375240821966448470"}},"outputId":"bd8f7bd2-908f-4ece-b56c-46af51f0018f"},"source":["#folder to save models\n","%cd '/content/drive/My Drive/NLP/Models/'\n","# Load the model params and the model\n","import json\n","with open(f'{model_name}_hparams.json', 'r') as fp:\n","    hparams = json.load(fp)\n","\n","model = transformer(**hparams)\n","model.load_weights(f'{model_name}.h5')\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP/Models\n","Model: \"transformer\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","inputs (InputLayer)             [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","dec_inputs (InputLayer)         [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n","__________________________________________________________________________________________________\n","encoder (Functional)            (None, None, 512)    55480320    inputs[0][0]                     \n","                                                                 enc_padding_mask[0][0]           \n","__________________________________________________________________________________________________\n","look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n","__________________________________________________________________________________________________\n","dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n","__________________________________________________________________________________________________\n","decoder (Functional)            (None, None, 512)    61790208    dec_inputs[0][0]                 \n","                                                                 encoder[0][0]                    \n","                                                                 look_ahead_mask[0][0]            \n","                                                                 dec_padding_mask[0][0]           \n","__________________________________________________________________________________________________\n","outputs (Dense)                 (None, None, 71418)  36637434    decoder[0][0]                    \n","==================================================================================================\n","Total params: 153,907,962\n","Trainable params: 153,907,962\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uFxOjOOIw64J"},"source":["def evaluate(sentence):\n","    sentence = normalizeString(sentence)\n","\n","    sentence = tf.expand_dims(\n","        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","    output = tf.expand_dims(START_TOKEN, 0)\n","\n","    for i in range(MAX_LENGTH):\n","        predictions = model(inputs=[sentence, output], training=False)\n","\n","        # select the last word from the seq_len dimension\n","        predictions = predictions[:, -1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        # return the result if the predicted_id is equal to the end token\n","        if tf.equal(predicted_id, END_TOKEN[0]):\n","            break\n","\n","        # concatenated the predicted_id to the output which is given to the decoder\n","        # as its input.\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0)\n","\n","\n","def predict(sentence):\n","    prediction = evaluate(sentence)\n","\n","    predicted_sentence = tokenizer.decode(\n","        [i for i in prediction if i < tokenizer.vocab_size])\n","\n","    #print('Input: {}'.format(sentence))\n","    #print('Output: {}'.format(predicted_sentence))\n","\n","    return predicted_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfcmN4cXxKYM"},"source":["def evaluateInput():\n","    input_sentence = ''\n","    while(1):\n","        try:\n","            # Get input sentence\n","            input_sentence = input('> ')\n","            # Check if it is quit case\n","            if input_sentence == 'q' or input_sentence == 'quit': break\n","            # Evaluate sentence\n","            predicted_sentence = predict(input_sentence)\n","            # print response sentence\n","\n","            print('Bot:', predicted_sentence)\n","\n","        except KeyError:\n","            print(\"Error: Encountered unknown word.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Wak8z2ZUvWq"},"source":["evaluateInput()"],"execution_count":null,"outputs":[]}]}